{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 4: Resampling Methods!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a.) Creating a Logistic Model for Default, based on Income and Balance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "library(ISLR)\n",
    "attach(Default)\n",
    "set.seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<ol class=list-inline>\n",
       "\t<li>'default'</li>\n",
       "\t<li>'student'</li>\n",
       "\t<li>'balance'</li>\n",
       "\t<li>'income'</li>\n",
       "</ol>\n"
      ],
      "text/latex": [
       "\\begin{enumerate*}\n",
       "\\item 'default'\n",
       "\\item 'student'\n",
       "\\item 'balance'\n",
       "\\item 'income'\n",
       "\\end{enumerate*}\n"
      ],
      "text/markdown": [
       "1. 'default'\n",
       "2. 'student'\n",
       "3. 'balance'\n",
       "4. 'income'\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "[1] \"default\" \"student\" \"balance\" \"income\" "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<ol class=list-inline>\n",
       "\t<li>10000</li>\n",
       "\t<li>4</li>\n",
       "</ol>\n"
      ],
      "text/latex": [
       "\\begin{enumerate*}\n",
       "\\item 10000\n",
       "\\item 4\n",
       "\\end{enumerate*}\n"
      ],
      "text/markdown": [
       "1. 10000\n",
       "2. 4\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "[1] 10000     4"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# some information about names\n",
    "names(Default)\n",
    "dim(Default)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "Call:  glm(formula = default ~ income + balance, family = \"binomial\", \n",
       "    data = Default)\n",
       "\n",
       "Coefficients:\n",
       "(Intercept)       income      balance  \n",
       " -1.154e+01    2.081e-05    5.647e-03  \n",
       "\n",
       "Degrees of Freedom: 9999 Total (i.e. Null);  9997 Residual\n",
       "Null Deviance:\t    2921 \n",
       "Residual Deviance: 1579 \tAIC: 1585"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#make a logistic model based on income and balance (remember to include family='binomial' to reduce anger and stress)\n",
    "logModelA = glm(default~income+balance, data=Default, family='binomial')\n",
    "logModelA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b.) Estimating Test Error of Model Using Validation Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead><tr><th></th><th scope=col>default</th><th scope=col>student</th><th scope=col>balance</th><th scope=col>income</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><th scope=row>2656</th><td>No        </td><td>Yes       </td><td>  80.59568</td><td>16145.64  </td></tr>\n",
       "\t<tr><th scope=row>3721</th><td>No        </td><td>Yes       </td><td>1470.47459</td><td>18621.17  </td></tr>\n",
       "\t<tr><th scope=row>5728</th><td>No        </td><td>Yes       </td><td>1150.54719</td><td>23705.95  </td></tr>\n",
       "\t<tr><th scope=row>9080</th><td>Yes       </td><td>No        </td><td>1856.91472</td><td>33445.62  </td></tr>\n",
       "\t<tr><th scope=row>2017</th><td>No        </td><td>Yes       </td><td> 689.51920</td><td>15600.05  </td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{r|llll}\n",
       "  & default & student & balance & income\\\\\n",
       "\\hline\n",
       "\t2656 & No         & Yes        &   80.59568 & 16145.64  \\\\\n",
       "\t3721 & No         & Yes        & 1470.47459 & 18621.17  \\\\\n",
       "\t5728 & No         & Yes        & 1150.54719 & 23705.95  \\\\\n",
       "\t9080 & Yes        & No         & 1856.91472 & 33445.62  \\\\\n",
       "\t2017 & No         & Yes        &  689.51920 & 15600.05  \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "| <!--/--> | default | student | balance | income | \n",
       "|---|---|---|---|---|\n",
       "| 2656 | No         | Yes        |   80.59568 | 16145.64   | \n",
       "| 3721 | No         | Yes        | 1470.47459 | 18621.17   | \n",
       "| 5728 | No         | Yes        | 1150.54719 | 23705.95   | \n",
       "| 9080 | Yes        | No         | 1856.91472 | 33445.62   | \n",
       "| 2017 | No         | Yes        |  689.51920 | 15600.05   | \n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "     default student balance    income  \n",
       "2656 No      Yes       80.59568 16145.64\n",
       "3721 No      Yes     1470.47459 18621.17\n",
       "5728 No      Yes     1150.54719 23705.95\n",
       "9080 Yes     No      1856.91472 33445.62\n",
       "2017 No      Yes      689.51920 15600.05"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead><tr><th></th><th scope=col>default</th><th scope=col>student</th><th scope=col>balance</th><th scope=col>income</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><th scope=row>1</th><td>No       </td><td>No       </td><td> 729.5265</td><td>44361.63 </td></tr>\n",
       "\t<tr><th scope=row>3</th><td>No       </td><td>No       </td><td>1073.5492</td><td>31767.14 </td></tr>\n",
       "\t<tr><th scope=row>4</th><td>No       </td><td>No       </td><td> 529.2506</td><td>35704.49 </td></tr>\n",
       "\t<tr><th scope=row>10</th><td>No       </td><td>No       </td><td>   0.0000</td><td>29275.27 </td></tr>\n",
       "\t<tr><th scope=row>17</th><td>No       </td><td>No       </td><td>   0.0000</td><td>50265.31 </td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{r|llll}\n",
       "  & default & student & balance & income\\\\\n",
       "\\hline\n",
       "\t1 & No        & No        &  729.5265 & 44361.63 \\\\\n",
       "\t3 & No        & No        & 1073.5492 & 31767.14 \\\\\n",
       "\t4 & No        & No        &  529.2506 & 35704.49 \\\\\n",
       "\t10 & No        & No        &    0.0000 & 29275.27 \\\\\n",
       "\t17 & No        & No        &    0.0000 & 50265.31 \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "| <!--/--> | default | student | balance | income | \n",
       "|---|---|---|---|---|\n",
       "| 1 | No        | No        |  729.5265 | 44361.63  | \n",
       "| 3 | No        | No        | 1073.5492 | 31767.14  | \n",
       "| 4 | No        | No        |  529.2506 | 35704.49  | \n",
       "| 10 | No        | No        |    0.0000 | 29275.27  | \n",
       "| 17 | No        | No        |    0.0000 | 50265.31  | \n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "   default student balance   income  \n",
       "1  No      No       729.5265 44361.63\n",
       "3  No      No      1073.5492 31767.14\n",
       "4  No      No       529.2506 35704.49\n",
       "10 No      No         0.0000 29275.27\n",
       "17 No      No         0.0000 50265.31"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<ol class=list-inline>\n",
       "\t<li>7000</li>\n",
       "\t<li>4</li>\n",
       "</ol>\n"
      ],
      "text/latex": [
       "\\begin{enumerate*}\n",
       "\\item 7000\n",
       "\\item 4\n",
       "\\end{enumerate*}\n"
      ],
      "text/markdown": [
       "1. 7000\n",
       "2. 4\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "[1] 7000    4"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<ol class=list-inline>\n",
       "\t<li>3000</li>\n",
       "\t<li>4</li>\n",
       "</ol>\n"
      ],
      "text/latex": [
       "\\begin{enumerate*}\n",
       "\\item 3000\n",
       "\\item 4\n",
       "\\end{enumerate*}\n"
      ],
      "text/markdown": [
       "1. 3000\n",
       "2. 4\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "[1] 3000    4"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#We want to split our data into training and testing.\n",
    "trainIndices = sample(10000, 7000)\n",
    "trainSet = Default[trainIndices,]\n",
    "testSet = Default[-trainIndices,]\n",
    "# for checks\n",
    "trainSet[0:5,]\n",
    "testSet[0:5,]\n",
    "dim(trainSet)\n",
    "dim(testSet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit a logistic model with just train\n",
    "logModelJustTrain = glm(default~income+balance, data=trainSet, family='binomial')\n",
    "logModelJustTrain.probs=predict(logModelJustTrain, testSet, type='response') #run the values through the model\n",
    "logModelJustTrain.preds=rep(0, dim(testSet)[1]) #create a vector of 'No's'\n",
    "logModelJustTrain.preds[logModelJustTrain.preds>.5]=1 #change any probabilities greater than .5 to 'yes'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead><tr><th></th><th scope=col>logModelJustTrain.probs</th><th scope=col>logModelJustTrain.preds</th><th scope=col>default</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><th scope=row>1</th><td>1.346030e-03</td><td>0           </td><td>No          </td></tr>\n",
       "\t<tr><th scope=row>3</th><td>7.497860e-03</td><td>0           </td><td>No          </td></tr>\n",
       "\t<tr><th scope=row>4</th><td>3.518271e-04</td><td>0           </td><td>No          </td></tr>\n",
       "\t<tr><th scope=row>10</th><td>1.435567e-05</td><td>0           </td><td>No          </td></tr>\n",
       "\t<tr><th scope=row>17</th><td>2.241233e-05</td><td>0           </td><td>No          </td></tr>\n",
       "\t<tr><th scope=row>25</th><td>1.401457e-03</td><td>0           </td><td>No          </td></tr>\n",
       "\t<tr><th scope=row>34</th><td>4.110772e-03</td><td>0           </td><td>No          </td></tr>\n",
       "\t<tr><th scope=row>43</th><td>1.481640e-02</td><td>0           </td><td>No          </td></tr>\n",
       "\t<tr><th scope=row>44</th><td>3.953969e-05</td><td>0           </td><td>No          </td></tr>\n",
       "\t<tr><th scope=row>46</th><td>2.594807e-04</td><td>0           </td><td>No          </td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{r|lll}\n",
       "  & logModelJustTrain.probs & logModelJustTrain.preds & default\\\\\n",
       "\\hline\n",
       "\t1 & 1.346030e-03 & 0            & No          \\\\\n",
       "\t3 & 7.497860e-03 & 0            & No          \\\\\n",
       "\t4 & 3.518271e-04 & 0            & No          \\\\\n",
       "\t10 & 1.435567e-05 & 0            & No          \\\\\n",
       "\t17 & 2.241233e-05 & 0            & No          \\\\\n",
       "\t25 & 1.401457e-03 & 0            & No          \\\\\n",
       "\t34 & 4.110772e-03 & 0            & No          \\\\\n",
       "\t43 & 1.481640e-02 & 0            & No          \\\\\n",
       "\t44 & 3.953969e-05 & 0            & No          \\\\\n",
       "\t46 & 2.594807e-04 & 0            & No          \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "| <!--/--> | logModelJustTrain.probs | logModelJustTrain.preds | default | \n",
       "|---|---|---|---|---|---|---|---|---|---|\n",
       "| 1 | 1.346030e-03 | 0            | No           | \n",
       "| 3 | 7.497860e-03 | 0            | No           | \n",
       "| 4 | 3.518271e-04 | 0            | No           | \n",
       "| 10 | 1.435567e-05 | 0            | No           | \n",
       "| 17 | 2.241233e-05 | 0            | No           | \n",
       "| 25 | 1.401457e-03 | 0            | No           | \n",
       "| 34 | 4.110772e-03 | 0            | No           | \n",
       "| 43 | 1.481640e-02 | 0            | No           | \n",
       "| 44 | 3.953969e-05 | 0            | No           | \n",
       "| 46 | 2.594807e-04 | 0            | No           | \n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "   logModelJustTrain.probs logModelJustTrain.preds default\n",
       "1  1.346030e-03            0                       No     \n",
       "3  7.497860e-03            0                       No     \n",
       "4  3.518271e-04            0                       No     \n",
       "10 1.435567e-05            0                       No     \n",
       "17 2.241233e-05            0                       No     \n",
       "25 1.401457e-03            0                       No     \n",
       "34 4.110772e-03            0                       No     \n",
       "43 1.481640e-02            0                       No     \n",
       "44 3.953969e-05            0                       No     \n",
       "46 2.594807e-04            0                       No     "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#print out the result of our predictions.\n",
    "result = data.frame(logModelJustTrain.probs, logModelJustTrain.preds, testSet['default'])\n",
    "result[0:10,]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Change default to map to 0 if no default, 1 to yes default\n",
    "dflt = rep(1:length(default))\n",
    "dflt[default=='No'] = 0\n",
    "dflt[default=='Yes'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "0.033"
      ],
      "text/latex": [
       "0.033"
      ],
      "text/markdown": [
       "0.033"
      ],
      "text/plain": [
       "[1] 0.033"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#This is the MSE (mean squared error)\n",
    "mean((dflt[-trainIndices]-logModelJustTrain.preds^2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c.) Repeat with different test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<ol class=list-inline>\n",
       "\t<li>0.033</li>\n",
       "\t<li>0.031</li>\n",
       "\t<li>0.027</li>\n",
       "\t<li>0.031</li>\n",
       "\t<li>0.0316666666666667</li>\n",
       "</ol>\n"
      ],
      "text/latex": [
       "\\begin{enumerate*}\n",
       "\\item 0.033\n",
       "\\item 0.031\n",
       "\\item 0.027\n",
       "\\item 0.031\n",
       "\\item 0.0316666666666667\n",
       "\\end{enumerate*}\n"
      ],
      "text/markdown": [
       "1. 0.033\n",
       "2. 0.031\n",
       "3. 0.027\n",
       "4. 0.031\n",
       "5. 0.0316666666666667\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "[1] 0.03300000 0.03100000 0.02700000 0.03100000 0.03166667"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "0.0328433333333333"
      ],
      "text/latex": [
       "0.0328433333333333"
      ],
      "text/markdown": [
       "0.0328433333333333"
      ],
      "text/plain": [
       "[1] 0.03284333"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Here, I create 100 different test sets and run logistic models for all of them\n",
    "y1 = rep(1:100)\n",
    "for (i in 1:100) {\n",
    "    set.seed(i)\n",
    "    trainIndices = sample(10000, 7000)\n",
    "    trainSet = Default[trainIndices,]\n",
    "    testSet = Default[-trainIndices,]\n",
    "\n",
    "    logModelJustTrain = glm(default~income+balance, data=trainSet, family='binomial')\n",
    "    logModelJustTrain.probs=predict(logModelJustTrain, testSet, type='response') #run the values through the model\n",
    "    logModelJustTrain.preds=rep(0, dim(testSet)[1]) #create a vector of 'No's'\n",
    "    logModelJustTrain.preds[logModelJustTrain.preds>.5]=1 #change any probabilities greater than .5 to 'yes'\n",
    "\n",
    "\n",
    "    y1[i] = (mean((dflt[-trainIndices]-logModelJustTrain.preds^2))) #MSE\n",
    "    }\n",
    "y1[0:5] #print out first 5 values\n",
    "mean(y1) #mean of means"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clearly, as we vary the training set values, we will get different MSE values, but they all seem to be around ~.0328. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### d.) Do the same, but considering 'student' as a predictor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<ol class=list-inline>\n",
       "\t<li>0.037</li>\n",
       "\t<li>0.0316666666666667</li>\n",
       "\t<li>0.033</li>\n",
       "\t<li>0.0313333333333333</li>\n",
       "\t<li>0.0326666666666667</li>\n",
       "</ol>\n"
      ],
      "text/latex": [
       "\\begin{enumerate*}\n",
       "\\item 0.037\n",
       "\\item 0.0316666666666667\n",
       "\\item 0.033\n",
       "\\item 0.0313333333333333\n",
       "\\item 0.0326666666666667\n",
       "\\end{enumerate*}\n"
      ],
      "text/markdown": [
       "1. 0.037\n",
       "2. 0.0316666666666667\n",
       "3. 0.033\n",
       "4. 0.0313333333333333\n",
       "5. 0.0326666666666667\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "[1] 0.03700000 0.03166667 0.03300000 0.03133333 0.03266667"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "0.0331933333333333"
      ],
      "text/latex": [
       "0.0331933333333333"
      ],
      "text/markdown": [
       "0.0331933333333333"
      ],
      "text/plain": [
       "[1] 0.03319333"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "0.000349999999999996"
      ],
      "text/latex": [
       "0.000349999999999996"
      ],
      "text/markdown": [
       "0.000349999999999996"
      ],
      "text/plain": [
       "[1] 0.00035"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "y2 = rep(1:100)\n",
    "for (i in 1:100) {\n",
    "    set.seed(i + 100)\n",
    "    trainIndices2 = sample(10000, 7000)\n",
    "    trainSet2 = Default[trainIndices2,]\n",
    "    testSet2 = Default[-trainIndices2,]\n",
    "\n",
    "    logModelJustTrain2 = glm(default~income+balance+student, data=trainSet2, family='binomial') #this time we add student\n",
    "    logModelJustTrain2.probs=predict(logModelJustTrain2, testSet2, type='response') #run the values through the model\n",
    "    logModelJustTrain2.preds=rep(0, dim(testSet2)[1]) #create a vector of 'No's'\n",
    "    logModelJustTrain2.preds[logModelJustTrain2.preds>.5]=1 #change any probabilities greater than .5 to 'yes'\n",
    "\n",
    "\n",
    "    y2[i]= (mean((dflt[-trainIndices2]-logModelJustTrain2.preds^2))) #MSE\n",
    "    }\n",
    "y2[0:5]\n",
    "mean(y2) #mean of the mean\n",
    "sqrt((mean(y2) - mean(y1))^2) #The difference in MSEs (from including student vs not including student)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The difference is extremely slight, but adding the student as a predictor increased the MSE by 0.0003499. To me, it doesn't seem like there would be a difference in adding or not adding 'student', but Occam's Razor tells us not to add it (and AIC/BIC too, I guess..)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now perform cross-validation on a simulated data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "set.seed(1)\n",
    "x=rnorm(100)\n",
    "y=x-2*x^2+rnorm(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead><tr><th scope=col>y</th><th scope=col>x</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><td>-2.0317092</td><td>-0.6264538</td></tr>\n",
       "\t<tr><td> 0.1583095</td><td> 0.1836433</td></tr>\n",
       "\t<tr><td>-3.1431006</td><td>-0.8356286</td></tr>\n",
       "\t<tr><td>-3.3365321</td><td> 1.5952808</td></tr>\n",
       "\t<tr><td>-0.5422276</td><td> 0.3295078</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{r|ll}\n",
       " y & x\\\\\n",
       "\\hline\n",
       "\t -2.0317092 & -0.6264538\\\\\n",
       "\t  0.1583095 &  0.1836433\\\\\n",
       "\t -3.1431006 & -0.8356286\\\\\n",
       "\t -3.3365321 &  1.5952808\\\\\n",
       "\t -0.5422276 &  0.3295078\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "y | x | \n",
       "|---|---|---|---|---|\n",
       "| -2.0317092 | -0.6264538 | \n",
       "|  0.1583095 |  0.1836433 | \n",
       "| -3.1431006 | -0.8356286 | \n",
       "| -3.3365321 |  1.5952808 | \n",
       "| -0.5422276 |  0.3295078 | \n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "  y          x         \n",
       "1 -2.0317092 -0.6264538\n",
       "2  0.1583095  0.1836433\n",
       "3 -3.1431006 -0.8356286\n",
       "4 -3.3365321  1.5952808\n",
       "5 -0.5422276  0.3295078"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<ol class=list-inline>\n",
       "\t<li>100</li>\n",
       "\t<li>2</li>\n",
       "</ol>\n"
      ],
      "text/latex": [
       "\\begin{enumerate*}\n",
       "\\item 100\n",
       "\\item 2\n",
       "\\end{enumerate*}\n"
      ],
      "text/markdown": [
       "1. 100\n",
       "2. 2\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "[1] 100   2"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#for checks\n",
    "data = data.frame(y, x)\n",
    "data[0:5,]\n",
    "dim(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a.) We are working with a 100x2 dataset, as seen the checks above. \n",
    "$$Y = B_0 + B_1X_1$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA0gAAANICAMAAADKOT/pAAADAFBMVEUAAAABAQECAgIDAwME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\nqn+p+oEAAAAASUVORK5CYII=",
      "text/plain": [
       "plot without title"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b.) This looks like a typical downward parabola with noise!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "library(boot)\n",
    "set.seed(37)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will fit the following models:\n",
    "$$\\text{1.) Y} = \\beta_0 + \\beta_1X + \\varepsilon$$\n",
    "\n",
    "$$\\text{2.) Y} = \\beta_0 + \\beta_1X + \\beta_2X^2 + \\varepsilon$$\n",
    "\n",
    "$$\\text{3.) Y} = \\beta_0 + \\beta_1X + \\beta_2X^2 + \\beta_3X^3 + \\varepsilon$$\n",
    "\n",
    "$$\\text{4.) Y} = \\beta_0 + \\beta_1X + \\beta_2X^2 + \\beta_3X^3 + \\beta_4X^4 + \\varepsilon$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] 7.288162\n",
      "[1] 0.9374236\n",
      "[1] 0.9566218\n",
      "[1] 0.9539049\n"
     ]
    }
   ],
   "source": [
    "#Create the models based on different degrees.\n",
    "lm.1 = glm(y~x, data=data)\n",
    "lm.2 = glm(y~x+I(x^2), data=data)\n",
    "lm.3 = glm(y~x+I(x^2)+I(x^3), data=data)\n",
    "# lm.4.1 = glm(y~poly(x, 4), data=data) this is same as below.\n",
    "lm.4 = glm(y~x+I(x^2)+I(x^3)+I(x^4), data=data)\n",
    "\n",
    "#Do K=N (LOOCV) on each model\n",
    "cv.1 = cv.glm(data, lm.1)\n",
    "cv.2 = cv.glm(data, lm.2)\n",
    "cv.3 = cv.glm(data, lm.3)\n",
    "cv.4 = cv.glm(data, lm.4)\n",
    "\n",
    "#Print out the MSE for each cross validation\n",
    "print (cv.1$delta[1])\n",
    "print (cv.2$delta[1])\n",
    "print (cv.3$delta[1])\n",
    "print (cv.4$delta[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c.) I'm gonna be completely honest here, I sat here for 20 minutes wondering why my MSE for my first model was so high. HAHAHAHA because the model is __A POLYNOMIAL__ what a surprise!!! But at the same time, I expected my 2 order model to have the lowest error, which it did. I just got really distracted by the large difference between my first model and the rest of them. I need to sleep."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] 7.288162\n",
      "[1] 0.9374236\n",
      "[1] 0.9566218\n",
      "[1] 0.9539049\n"
     ]
    }
   ],
   "source": [
    "set.seed(39)\n",
    "#Create the models based on different degrees.\n",
    "lm.1 = glm(y~x, data=data)\n",
    "lm.2 = glm(y~x+I(x^2), data=data)\n",
    "lm.3 = glm(y~x+I(x^2)+I(x^3), data=data)\n",
    "# lm.4.1 = glm(y~poly(x, 4), data=data) this is same as below.\n",
    "lm.4 = glm(y~x+I(x^2)+I(x^3)+I(x^4), data=data)\n",
    "\n",
    "#Do K=N (LOOCV) on each model\n",
    "cv.1 = cv.glm(data, lm.1)\n",
    "cv.2 = cv.glm(data, lm.2)\n",
    "cv.3 = cv.glm(data, lm.3)\n",
    "cv.4 = cv.glm(data, lm.4)\n",
    "\n",
    "#Print out the MSE for each cross validation\n",
    "print (cv.1$delta[1])\n",
    "print (cv.2$delta[1])\n",
    "print (cv.3$delta[1])\n",
    "print (cv.4$delta[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "d.) Because of the nature of cross validation, we should get the same results for the MSE, regardless of what seed we use. We're looking at the entire data set and taking an average of all of the different train/test combinations. Unless we change the value of K, we should always get the same values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "e.) Again, we should expect polynomial degree 2 to have the lowest MSE, since the original dataset was based on a X^2 factor. This was verified through the cross validation MSE, which was the lowest at polynomial degree 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"X P-VALUES\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "1.30929972880896e-08"
      ],
      "text/latex": [
       "1.30929972880896e-08"
      ],
      "text/markdown": [
       "1.30929972880896e-08"
      ],
      "text/plain": [
       "[1] 1.3093e-08"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "0.0192384645072811"
      ],
      "text/latex": [
       "0.0192384645072811"
      ],
      "text/markdown": [
       "0.0192384645072811"
      ],
      "text/plain": [
       "[1] 0.01923846"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"X^2 P-VALUES\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "0.630861286236613"
      ],
      "text/latex": [
       "0.630861286236613"
      ],
      "text/markdown": [
       "0.630861286236613"
      ],
      "text/plain": [
       "[1] 0.6308613"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "2.40328674176305e-15"
      ],
      "text/latex": [
       "2.40328674176305e-15"
      ],
      "text/markdown": [
       "2.40328674176305e-15"
      ],
      "text/plain": [
       "[1] 2.403287e-15"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "4.58433015088097e-44"
      ],
      "text/latex": [
       "4.58433015088097e-44"
      ],
      "text/markdown": [
       "4.58433015088097e-44"
      ],
      "text/plain": [
       "[1] 4.58433e-44"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"X^3 P-VALUES\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "0.607953796738618"
      ],
      "text/latex": [
       "0.607953796738618"
      ],
      "text/markdown": [
       "0.607953796738618"
      ],
      "text/plain": [
       "[1] 0.6079538"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "1.08934983369184e-06"
      ],
      "text/latex": [
       "1.08934983369184e-06"
      ],
      "text/markdown": [
       "1.08934983369184e-06"
      ],
      "text/plain": [
       "[1] 1.08935e-06"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "5.87344377541289e-43"
      ],
      "text/latex": [
       "5.87344377541289e-43"
      ],
      "text/markdown": [
       "5.87344377541289e-43"
      ],
      "text/plain": [
       "[1] 5.873444e-43"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "0.784399039902434"
      ],
      "text/latex": [
       "0.784399039902434"
      ],
      "text/markdown": [
       "0.784399039902434"
      ],
      "text/plain": [
       "[1] 0.784399"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"X^4 P-VALUES\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "0.264003389109833"
      ],
      "text/latex": [
       "0.264003389109833"
      ],
      "text/markdown": [
       "0.264003389109833"
      ],
      "text/plain": [
       "[1] 0.2640034"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "5.17432635724482e-07"
      ],
      "text/latex": [
       "5.17432635724482e-07"
      ],
      "text/markdown": [
       "5.17432635724482e-07"
      ],
      "text/plain": [
       "[1] 5.174326e-07"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "4.57522935340721e-17"
      ],
      "text/latex": [
       "4.57522935340721e-17"
      ],
      "text/markdown": [
       "4.57522935340721e-17"
      ],
      "text/plain": [
       "[1] 4.575229e-17"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "0.892228834068777"
      ],
      "text/latex": [
       "0.892228834068777"
      ],
      "text/markdown": [
       "0.892228834068777"
      ],
      "text/plain": [
       "[1] 0.8922288"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "0.193095643040333"
      ],
      "text/latex": [
       "0.193095643040333"
      ],
      "text/markdown": [
       "0.193095643040333"
      ],
      "text/plain": [
       "[1] 0.1930956"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print('X P-VALUES')\n",
    "coef(summary(lm.1))[1, 4] #p value of intercept\n",
    "coef(summary(lm.1))[2, 4] # x\n",
    "\n",
    "print('X^2 P-VALUES')\n",
    "coef(summary(lm.2))[1, 4] #p value of intercept\n",
    "coef(summary(lm.2))[2, 4] # x\n",
    "coef(summary(lm.2))[3, 4] # x^2\n",
    "\n",
    "print('X^3 P-VALUES')\n",
    "coef(summary(lm.3))[1, 4] #p value of intercept\n",
    "coef(summary(lm.3))[2, 4] # x\n",
    "coef(summary(lm.3))[3, 4] # x^2\n",
    "coef(summary(lm.3))[4, 4] # x^3\n",
    "\n",
    "print('X^4 P-VALUES')\n",
    "coef(summary(lm.4))[1, 4] #p value of intercept\n",
    "coef(summary(lm.4))[2, 4] # x\n",
    "coef(summary(lm.4))[3, 4] # x^2\n",
    "coef(summary(lm.4))[4, 4] # x^3\n",
    "coef(summary(lm.4))[5, 4] # x^4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "f.) Yup! The 2nd order model also has the lowest (best) p-values also. It seems like the x-intercept isn't particularly significant, but for the $x^2$ orders and above, the $x^2$ term is also the highest. Everything agrees and works out!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.4.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
