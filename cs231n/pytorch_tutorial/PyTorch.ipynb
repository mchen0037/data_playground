{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First, let's create the NN using numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "N = batch size, D is input dimension\n",
    "H is hidden dimension, D_out is output dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "N, D_in, H, D_out = 1000, 4000, 100, 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create some input and output data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 10 labels for each 64x1000 instances\n",
    "x = np.random.randn(N, D_in)\n",
    "y = np.random.randn(N, D_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Randomly initialize the weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "w1 = np.random.randn(D_in, H)\n",
    "w2 = np.random.randn(H, D_out)\n",
    "\n",
    "learning_rate = 1e-6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss at iteration  0 :  30875644.820042342\n",
      "loss at iteration  100 :  186.27551934551707\n",
      "loss at iteration  200 :  0.2406082901832312\n",
      "loss at iteration  300 :  0.0006639500610564422\n",
      "loss at iteration  400 :  2.4600319804153245e-06\n"
     ]
    }
   ],
   "source": [
    "for t in range(500):\n",
    "    # Forward pass: compute predicted y\n",
    "    h = x.dot(w1)\n",
    "    h_relu = np.maximum(h, 0)\n",
    "    y_pred = h_relu.dot(w2)\n",
    "    \n",
    "    # Compute the loss using RSS\n",
    "    loss = np.square(y_pred-y).sum()\n",
    "    if (t%100 == 0):\n",
    "        print(\"loss at iteration \", t, \": \", loss)\n",
    "    \n",
    "    # Back propogate to compute the gradients with resepct to loss\n",
    "    grad_y_pred = 2.0 * (y_pred - y)\n",
    "    grad_w2 = h_relu.T.dot(grad_y_pred)\n",
    "    grad_h_relu = grad_y_pred.dot(w2.T)\n",
    "    grad_h = grad_h_relu.copy()\n",
    "    grad_h[h<0] = 0\n",
    "    grad_w1 = x.T.dot(grad_h)\n",
    "    \n",
    "    # Update weights\n",
    "    w1 -= learning_rate * grad_w1\n",
    "    w2 -= learning_rate * grad_w2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now we can do the same thing but using torch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dtype = torch.float\n",
    "device = torch.device(\"cpu\")\n",
    "# device = torch.device(\"cuda\")\n",
    "\n",
    "# Random numbers.\n",
    "N, D_in, H, D_out = 1000, 4000, 100, 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create random input and output data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x = torch.randn(N, D_in, device=device, dtype=dtype)\n",
    "y = torch.randn(N, D_out, device=device, dtype=dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Randomly init the weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "w1 = torch.randn(D_in, H, device=device, dtype=dtype, requires_grad=True)\n",
    "w2 = torch.randn(H, D_out, device=device, dtype=dtype, requires_grad=True)\n",
    "\n",
    "learning_rate=1e-8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss at iteration  0  :  2542859520.0\n",
      "loss at iteration  200  :  862070.75\n",
      "loss at iteration  400  :  13489.583984375\n",
      "loss at iteration  600  :  9117.712890625\n",
      "loss at iteration  800  :  9086.10546875\n",
      "CPU time took was:  8.837102174758911\n"
     ]
    }
   ],
   "source": [
    "first = time.time()\n",
    "for t in range(1000):\n",
    "    # Forward pass \n",
    "    y_pred = x.mm(w1).clamp(min=0).mm(w2)\n",
    "#     print(y_pred.size())\n",
    "#     print(y.size())\n",
    "    \n",
    "    # Compute loss\n",
    "    loss = (y_pred - y).pow(2).sum()\n",
    "    if t % 200 == 0:\n",
    "        print(\"loss at iteration \", t , \" : \" , loss.item())\n",
    "    \n",
    "    # Backprop to compute gradients - autograd will compute the gradient of loss\n",
    "    # with resepct to all tensors with requires_grad=True.\n",
    "    # w1.grad and w2.grad will be tensors holding the gradient of the loss wrt to w1 w2\n",
    "    loss.backward()\n",
    "    \n",
    "    # Update the weights using gradient descent. wrap in torch.no_grad() because \n",
    "    # weights have requires_grad=True; but we don't need to track this. \n",
    "    \n",
    "    with torch.no_grad():\n",
    "        w1 -= learning_rate * w1.grad\n",
    "        w2 -= learning_rate * w2.grad\n",
    "        \n",
    "        # set the gradients back to zero \n",
    "        w1.grad.zero_()\n",
    "        w2.grad.zero_()\n",
    "end = time.time()\n",
    "print(\"CPU time took was: \", end-first)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://stackoverflow.com/questions/53325418/pytorch-speed-comparison-gpu-slower-than-cpu\n",
    "\n",
    "In the low dimensional problem, I was trying to solve the NN on the GPU: the GMC data was 200x13--relatively small. I was also trying to iterate over more loops because I thought this would make the problem longer and I would be able to better see the effects of GPU vs CPU. I did, but not in the way that I thought I would.\n",
    "\n",
    "The GPU is opitmized for doing small, parallelized problems, i.e solving the gradients in a HIGH dimensional problem. In these, some of the partial derivatives are not dependent on each other so we can paralellize them. However, in a small network architecture, running ```loss.backward()``` would parallelize it, but it was not paralellizing a lot because there was not much to parallelize.\n",
    "\n",
    "I came back to this toy data set and increased the number of dimensons. Increasing the number of dimensions increases the number of parallelization operations we need to compute. Increasing the number of iterations does not really mean anything in this problem, so iterating it over 100000 operations or whatever I did just made the problem in general slower, not how many operations I was sending to the GPU. As we can see, the CPU took much longer this time (with fewer iterations) and then GPU took much faster. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dtype = torch.float\n",
    "# device = torch.device(\"cpu\")\n",
    "device = torch.device(\"cuda\")\n",
    "\n",
    "x = torch.randn(N, D_in, device=device, dtype=dtype)\n",
    "y = torch.randn(N, D_out, device=device, dtype=dtype)\n",
    "\n",
    "w1 = torch.randn(D_in, H, device=device, dtype=dtype, requires_grad=True)\n",
    "w2 = torch.randn(H, D_out, device=device, dtype=dtype, requires_grad=True)\n",
    "\n",
    "learning_rate=2e-8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss at iteration  0  :  1491390592.0\n",
      "loss at iteration  200  :  67001.0625\n",
      "loss at iteration  400  :  8984.865234375\n",
      "loss at iteration  600  :  8931.345703125\n",
      "loss at iteration  800  :  8931.2744140625\n",
      "GPU time took was:  0.927548885345459\n"
     ]
    }
   ],
   "source": [
    "first = time.time()\n",
    "for t in range(1000):\n",
    "    # Forward pass \n",
    "    y_pred = x.mm(w1).clamp(min=0).mm(w2)\n",
    "#     print(y_pred.size())\n",
    "#     print(y.size())\n",
    "    \n",
    "    # Compute loss\n",
    "    loss = (y_pred - y).pow(2).sum()\n",
    "    if t % 200 == 0:\n",
    "        print(\"loss at iteration \", t , \" : \" , loss.item())\n",
    "    \n",
    "    # Backprop to compute gradients - autograd will compute the gradient of loss\n",
    "    # with resepct to all tensors with requires_grad=True.\n",
    "    # w1.grad and w2.grad will be tensors holding the gradient of the loss wrt to w1 w2\n",
    "    loss.backward()\n",
    "    \n",
    "    # Update the weights using gradient descent. wrap in torch.no_grad() because \n",
    "    # weights have requires_grad=True; but we don't need to track this. \n",
    "    \n",
    "    with torch.no_grad():\n",
    "        w1 -= learning_rate * w1.grad\n",
    "        w2 -= learning_rate * w2.grad\n",
    "        \n",
    "        # set the gradients back to zero \n",
    "        w1.grad.zero_()\n",
    "        w2.grad.zero_()\n",
    "end = time.time()\n",
    "print(\"GPU time took was: \", end-first)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
